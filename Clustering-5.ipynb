{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be588de-155a-4866-952b-859c76ff136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1):-\n",
    "A contingency matrix, also known as a confusion matrix or an error matrix, is a table used to evaluate the performance of a classification model. \n",
    "It provides a comprehensive summary of how well the model's predictions align with the actual class labels in a classification task. The contingency \n",
    "matrix is particularly useful when dealing with supervised learning problems where you have a set of true class labels and a set of predicted class \n",
    "labels.\n",
    "\n",
    "The contingency matrix is organized as follows:\n",
    "\n",
    "Rows represent the actual (true) class labels.\n",
    "Columns represent the predicted class labels.\n",
    "The main components of a contingency matrix are:\n",
    "\n",
    "True Positives (TP): The number of data points that were correctly classified as positive (belonging to the positive class).\n",
    "\n",
    "True Negatives (TN): The number of data points that were correctly classified as negative (not belonging to the positive class).\n",
    "\n",
    "False Positives (FP): The number of data points that were incorrectly classified as positive when they actually belong to the negative class \n",
    "(Type I error).\n",
    "\n",
    "False Negatives (FN): The number of data points that were incorrectly classified as negative when they actually belong to the positive class\n",
    "(Type II error).\n",
    "\n",
    "Here's a visual representation of a contingency matrix:\n",
    "\n",
    "                    | Predicted Positive | Predicted Negative |\n",
    "Actual Positive     |        TP          |        FN          |\n",
    "Actual Negative     |        FP          |        TN          |\n",
    "\n",
    "How the Contingency Matrix is Used to Evaluate Model Performance:\n",
    "\n",
    "Accuracy: The accuracy of the classification model can be calculated as (TP + TN) / (TP + TN + FP + FN). It measures the overall correctness of the\n",
    "predictions.\n",
    "\n",
    "Precision (Positive Predictive Value): Precision is calculated as TP / (TP + FP). It represents the proportion of true positive predictions among all\n",
    "positive predictions. High precision indicates that when the model predicts the positive class, it's usually correct.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall is calculated as TP / (TP + FN). It measures the proportion of true positive predictions among all\n",
    "actual positive instances. High recall indicates that the model is good at capturing positive instances.\n",
    "\n",
    "F1-Score: The F1-score is the harmonic mean of precision and recall and is calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides\n",
    "a balance between precision and recall, which can be useful when dealing with imbalanced datasets.\n",
    "\n",
    "Specificity (True Negative Rate): Specificity is calculated as TN / (TN + FP). It measures the proportion of true negative predictions among all\n",
    "actual negative instances.\n",
    "\n",
    "False Positive Rate (FPR): FPR is calculated as FP / (FP + TN). It quantifies the rate of false alarms or Type I errors.\n",
    "\n",
    "True Negative Rate (TNR): TNR is calculated as TN / (TN + FP). It measures the ability of the model to correctly classify negative instances.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve: The ROC curve is a graphical representation of the trade-off between sensitivity (recall) and \n",
    "specificity as the classification threshold varies. It helps assess the model's ability to discriminate between classes.\n",
    "\n",
    "Area Under the ROC Curve (AUC-ROC): The AUC-ROC provides a single scalar value that summarizes the ROC curve's performance. It quantifies the model's \n",
    "overall ability to distinguish between positive and negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca17587-9917-4eda-adb0-b54ec3a94b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2):-\n",
    "A pair confusion matrix, also known as a pairwise confusion matrix or a confusion matrix for pairwise classification, is a variation of the regular \n",
    "confusion matrix used in specific situations, particularly in multi-class classification problems. The key difference between the two lies in the way\n",
    "they handle class pairs or combinations.\n",
    "\n",
    "Regular Confusion Matrix:\n",
    "\n",
    "In a regular confusion matrix, each row corresponds to a true class, and each column corresponds to a predicted class.\n",
    "The cells in the matrix represent counts of data points that fall into each combination of true class and predicted class.\n",
    "It provides a detailed view of the model's performance for each individual class.\n",
    "\n",
    "Pair Confusion Matrix:\n",
    "\n",
    "In a pair confusion matrix, the focus is on comparing pairs of classes rather than individual classes.\n",
    "The matrix is typically square, with each row and column representing one of the class pairs (combinations).\n",
    "The cells in the matrix represent counts of data points that belong to one class pair (e.g., class A vs. class B) and are correctly or incorrectly \n",
    "classified as such.\n",
    "\n",
    "The pair confusion matrix is particularly useful in certain situations:\n",
    "\n",
    "Multi-Class Classification with Imbalanced Data: In multi-class problems where classes may be imbalanced, a regular confusion matrix might emphasize\n",
    "the majority classes and obscure the performance on minority classes. Pairwise confusion matrices can highlight the performance of interest, such as \n",
    "distinguishing between a minority class and a specific majority class.\n",
    "\n",
    "Class Pair Evaluation: When you are specifically interested in evaluating the model's performance on discriminating between pairs of classes, a pair\n",
    "confusion matrix provides a clearer view of how well the model distinguishes between class pairs.\n",
    "\n",
    "One-vs-One Classification: In some multi-class classification algorithms, such as one-vs-one (OvO) classifiers, the model is trained on binary \n",
    "subproblems for each pair of classes. Pair confusion matrices align well with the OvO strategy, allowing you to assess the model's performance \n",
    "for each class pair independently.\n",
    "\n",
    "Reducing Dimensionality: Pair confusion matrices can be used to reduce the dimensionality of the evaluation in multi-class problems. Instead of\n",
    "analyzing performance across many individual classes, you focus on the comparisons that matter most.\n",
    "\n",
    "Here's a simplified example to illustrate the difference:\n",
    "\n",
    "Regular Confusion Matrix (for a 3-class problem):\n",
    "                Predicted Class 1 | Predicted Class 2 | Predicted Class 3\n",
    "Actual Class 1         TP1                FN1                FN2\n",
    "Actual Class 2         FN3                TP2                FN4\n",
    "Actual Class 3         FN5                FN6                TP3\n",
    "\n",
    "Pair Confusion Matrix (for the same 3-class problem):\n",
    "    \n",
    "                  Class 1 vs. Class 2 | Class 1 vs. Class 3 | Class 2 vs. Class 3\n",
    "Class 1 vs. Class 2           TP1                FN1                FN2\n",
    "Class 1 vs. Class 3           FN3                TP2                FN4\n",
    "Class 2 vs. Class 3           FN5                FN6                TP3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a018936-1e17-4265-9af1-43eb61608175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3):-\n",
    "In the context of natural language processing (NLP), an extrinsic measure is an evaluation metric or methodology that assesses the performance of a \n",
    "language model or NLP system based on its ability to perform a specific downstream task. Unlike intrinsic measures, which evaluate the language model\n",
    "in isolation using proxy tasks (e.g., language modeling perplexity), extrinsic measures focus on evaluating the model's performance within a real-world\n",
    "application or use case.\n",
    "\n",
    "Here's how extrinsic measures are typically used to evaluate the performance of language models in NLP:\n",
    "\n",
    "Downstream Task: An extrinsic measure starts with a specific downstream NLP task or application, such as sentiment analysis, named entity recognition,\n",
    "machine translation, text summarization, question answering, or any other task that the language model is intended to support.\n",
    "\n",
    "Task-Specific Evaluation: The language model is integrated into or used as a component of the downstream task. For example, a sentiment analysis model \n",
    "may use a pre-trained language model to perform sentiment classification on a set of user reviews.\n",
    "\n",
    "Evaluation Metrics: To assess the performance of the language model within the task, task-specific evaluation metrics are employed. These metrics vary\n",
    "depending on the nature of the task and may include accuracy, F1-score, BLEU score, ROUGE score, perplexity, and more.\n",
    "\n",
    "Benchmark Datasets: Benchmark datasets containing annotated or labeled data for the specific task are used to evaluate the language model's \n",
    "performance. These datasets are carefully curated and typically include a variety of examples to ensure a robust evaluation.\n",
    "\n",
    "Comparative Analysis: The language model's performance on the downstream task is compared to that of other models or baselines. This allows \n",
    "researchers and practitioners to assess how well the language model performs relative to existing solutions or alternative approaches.\n",
    "\n",
    "Fine-Tuning and Adaptation: In many cases, pre-trained language models are fine-tuned or adapted to the specific downstream task to improve their\n",
    "performance. This fine-tuning process may involve updating model weights, training additional task-specific layers, or using transfer learning\n",
    "techniques.\n",
    "\n",
    "Real-World Utility: Ultimately, the goal of extrinsic measures is to assess the real-world utility of the language model. Researchers and \n",
    "practitioners aim to determine whether the language model can effectively and accurately solve the task it was designed for, and whether it provides\n",
    "practical value in applications such as chatbots, recommendation systems, search engines, and more.\n",
    "\n",
    "Examples of extrinsic measures in NLP include:\n",
    "\n",
    "Accuracy and F1-score for text classification tasks (e.g., sentiment analysis).\n",
    "BLEU and METEOR scores for machine translation tasks.\n",
    "ROUGE and METEOR scores for text summarization tasks.\n",
    "Precision, recall, and F1-score for named entity recognition tasks.\n",
    "Mean Average Precision (MAP) for information retrieval tasks.\n",
    "Extrinsic measures are highly valuable for assessing the real-world applicability and effectiveness of language models because they directly evaluate \n",
    "the model's performance within specific applications or use cases. This allows NLP researchers and practitioners to make informed decisions about model\n",
    "selection, fine-tuning strategies, and deployment based on task-specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0274f-c7a2-4f3f-b523-1a2fe600fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4):-\n",
    "In the context of machine learning and evaluation of models, intrinsic and extrinsic measures are two different approaches used to assess model\n",
    "performance and effectiveness. Here's how they differ:\n",
    "\n",
    "Intrinsic Measures:\n",
    "\n",
    "Definition: Intrinsic measures, also known as proxy or internal measures, evaluate the quality of a model based on its performance on a surrogate \n",
    "task or a metric that doesn't directly relate to the model's intended real-world application.\n",
    "\n",
    "Use: Intrinsic measures are often used during model development and training to monitor and improve model performance. They provide feedback on how \n",
    "well a model is learning its training data and optimizing its parameters.\n",
    "\n",
    "Examples:\n",
    "\n",
    "In natural language processing (NLP), intrinsic measures could include perplexity or cross-entropy for language models, which assess the model's \n",
    "ability to predict the next word in a sequence.\n",
    "In computer vision, intrinsic measures could involve metrics like mean squared error (MSE) for image denoising tasks.\n",
    "Purpose: Intrinsic measures are primarily used as diagnostic tools for model development. They help researchers and practitioners fine-tune model \n",
    "architectures, hyperparameters, and training strategies.\n",
    "\n",
    "Extrinsic Measures:\n",
    "\n",
    "Definition: Extrinsic measures, also known as application or task-specific measures, assess a model's performance based on its ability to solve a\n",
    "real-world application or task. These measures evaluate the utility and effectiveness of the model in practical use cases.\n",
    "\n",
    "Use: Extrinsic measures are employed to determine how well a model performs in a specific application or scenario. They are used to measure the \n",
    "model's real-world impact and whether it meets the desired objectives.\n",
    "\n",
    "Examples:\n",
    "\n",
    "In NLP, extrinsic measures could include accuracy, F1-score, or BLEU score for sentiment analysis, named entity recognition, or machine translation \n",
    "tasks, respectively.\n",
    "In computer vision, extrinsic measures could include accuracy, mean average precision (MAP), or intersection over union (IoU) for object detection,\n",
    "image classification, or semantic segmentation tasks.\n",
    "Purpose: Extrinsic measures focus on the practical utility of the model in real-world applications. They are used to make decisions about model\n",
    "deployment and assess its effectiveness in solving specific tasks.\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "Task Relevance: Intrinsic measures are often not directly related to the real-world application the model is intended for, while extrinsic measures \n",
    "evaluate the model's performance within the context of that application.\n",
    "\n",
    "Use Case: Intrinsic measures are used mainly for model development, debugging, and fine-tuning. Extrinsic measures are used to evaluate the model's\n",
    "practical usefulness and impact on specific tasks or applications.\n",
    "\n",
    "Evaluation Metrics: Intrinsic measures use specific surrogate metrics that might not directly correspond to the task's success. Extrinsic measures \n",
    "use task-specific evaluation metrics that are relevant to the application.\n",
    "\n",
    "In summary, intrinsic measures are employed during model development to assess how well the model learns from data, while extrinsic measures are used \n",
    "to evaluate the model's effectiveness in real-world tasks or applications. Both types of measures are valuable in machine learning, as intrinsic\n",
    "measures help improve model quality, and extrinsic measures determine a model's real-world utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1eb09-e766-47f9-baf7-5acf175853f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5):-\n",
    "A confusion matrix is a fundamental tool in machine learning and classification tasks. Its purpose is to provide a detailed and structured way to \n",
    "evaluate the performance of a classification model by comparing its predictions with the actual class labels. It is particularly useful for \n",
    "understanding both the strengths and weaknesses of a model. Here's how a confusion matrix is constructed and used:\n",
    "\n",
    "Construction of a Confusion Matrix:\n",
    "\n",
    "In a binary classification problem (two classes: positive and negative), a confusion matrix is organized as follows:\n",
    "\n",
    "True Positives (TP): The number of instances that were correctly classified as positive.\n",
    "True Negatives (TN): The number of instances that were correctly classified as negative.\n",
    "False Positives (FP): The number of instances that were incorrectly classified as positive when they were actually negative (Type I error).\n",
    "False Negatives (FN): The number of instances that were incorrectly classified as negative when they were actually positive (Type II error).\n",
    "\n",
    "Here's a visual representation:\n",
    "                 Predicted Positive | Predicted Negative\n",
    "Actual Positive     TP              | FN\n",
    "Actual Negative     FP              | TN\n",
    "\n",
    "In multi-class classification, where there are more than two classes, a confusion matrix is extended to capture the performance for each class.\n",
    "\n",
    "Using a Confusion Matrix to Identify Strengths and Weaknesses:\n",
    "\n",
    "Accuracy Assessment: The overall accuracy of the model can be calculated by adding up the true positives and true negatives and dividing by the total\n",
    "number of instances (TP + TN) / (TP + TN + FP + FN). High accuracy indicates overall good performance.\n",
    "\n",
    "Precision and Recall (Sensitivity):\n",
    "\n",
    "Precision (Positive Predictive Value) measures the proportion of true positive predictions among all positive predictions. It's calculated as \n",
    "TP / (TP + FP). High precision indicates that when the model predicts the positive class, it's usually correct.\n",
    "Recall (Sensitivity or True Positive Rate) measures the proportion of true positive predictions among all actual positive instances. \n",
    "It's calculated as TP / (TP + FN). High recall indicates that the model is good at capturing positive instances.\n",
    "F1-Score: The F1-score is the harmonic mean of precision and recall and is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "It provides a balance between precision and recall.\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures the proportion of true negative predictions among all actual negative instances and is \n",
    "calculated as TN / (TN + FP).\n",
    "\n",
    "False Positive Rate (FPR): FPR quantifies the rate of false alarms or Type I errors and is calculated as FP / (FP + TN).\n",
    "\n",
    "By analyzing the values in a confusion matrix and the derived metrics, you can identify various strengths and weaknesses of a model:\n",
    "\n",
    "Strengths: High TP, TN, precision, recall, and accuracy indicate that the model performs well in correctly classifying instances.\n",
    "Weaknesses: High FP, FN, low precision, low recall, and low accuracy indicate areas where the model needs improvement.\n",
    "For example, if you observe high FN values, it means the model is missing some positive instances and should improve its recall. Conversely, if\n",
    "there are high FP values, the model may need better precision.\n",
    "\n",
    "In summary, a confusion matrix is a valuable tool for evaluating the performance of classification models and pinpointing their strengths and \n",
    "weaknesses. It helps practitioners understand how well a model is performing, diagnose specific issues, and make informed decisions about model\n",
    "improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78897497-c860-4f45-bd74-a08714d852b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6):-\n",
    "Unsupervised learning algorithms, which include clustering and dimensionality reduction techniques, are typically evaluated using intrinsic measures.\n",
    "These measures assess the quality of the model's output without relying on external labels or ground truth. Here are some common intrinsic measures \n",
    "used to evaluate unsupervised learning algorithms and how they can be interpreted:\n",
    "\n",
    "Silhouette Score:\n",
    "\n",
    "Interpretation: The silhouette score quantifies how similar each data point is to its own cluster compared to other clusters. It ranges from -1 \n",
    "(poor clustering) to +1 (dense, well-separated clusters) with 0 indicating overlapping clusters.\n",
    "Use: Higher silhouette scores indicate better cluster separation and cohesion, suggesting that the clustering is more meaningful.\n",
    "\n",
    "Davies-Bouldin Index:\n",
    "Interpretation: The Davies-Bouldin index measures the average similarity between each cluster and its most similar cluster. Lower values indicate\n",
    "better clustering with more distinct clusters.\n",
    "Use: A lower Davies-Bouldin index suggests more clearly separated clusters.\n",
    "\n",
    "Calinski-Harabasz Index (Variance Ratio Criterion):\n",
    "Interpretation: The Calinski-Harabasz index measures the ratio of between-cluster variance to within-cluster variance. Higher values indicate better\n",
    "clustering, as it suggests that clusters are more distinct from each other.\n",
    "Use: A higher Calinski-Harabasz index implies better cluster quality.\n",
    "\n",
    "Dunn Index:\n",
    "Interpretation: The Dunn index assesses the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance.\n",
    "Higher values indicate better clustering with smaller within-cluster variance and larger between-cluster separation.\n",
    "Use: A higher Dunn index suggests improved cluster quality.\n",
    "\n",
    "Inertia (Within-Cluster Sum of Squares):\n",
    "Interpretation: Inertia measures the sum of squared distances from each data point to its cluster's centroid. Lower inertia indicates that data\n",
    "points are closer to their cluster centroids, implying better clustering.\n",
    "Use: Minimizing inertia is a common objective in algorithms like k-means.\n",
    "\n",
    "Explained Variance Ratio (PCA):\n",
    "Interpretation: In principal component analysis (PCA), the explained variance ratio quantifies the proportion of the total variance in the data that\n",
    "is explained by each principal component. It helps determine the dimensionality required to capture most of the data's variability.\n",
    "Use: Higher explained variance ratios indicate that a smaller number of principal components capture a significant portion of the data's variance, \n",
    "suggesting a more effective dimensionality reduction.\n",
    "\n",
    "Gap Statistic:\n",
    "Interpretation: The gap statistic compares the within-cluster dispersion of the data to a reference null distribution. It measures how far the\n",
    "observed clustering deviates from random clustering. A higher gap statistic suggests that the clustering is more significant than random.\n",
    "Use: The gap statistic helps determine whether the clustering structure is meaningful compared to a random baseline.\n",
    "\n",
    "Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI):\n",
    "Interpretation: ARI and NMI are measures of clustering similarity between the true labels (if available) and the cluster assignments. They assess \n",
    "how well the clustering captures the underlying structure.\n",
    "Use: Higher ARI and NMI values indicate better agreement between the true labels and the clustering results.\n",
    "\n",
    "Proportion of Explained Variance (PCA):\n",
    "Interpretation: In PCA, this metric represents the proportion of the total variance in the data explained by a selected number of principal \n",
    "components. It helps determine how much dimensionality reduction is appropriate.\n",
    "Use: Higher proportions of explained variance suggest that fewer principal components capture most of the data's variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ea66c-e4ed-46f1-8492-351354c8339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7):-\n",
    "Using accuracy as the sole evaluation metric for classification tasks has several limitations, and it may not provide a complete picture of a model's \n",
    "performance. Here are some common limitations of accuracy and how they can be addressed:\n",
    "\n",
    "Imbalanced Datasets:\n",
    "\n",
    "Limitation: Accuracy can be misleading when dealing with imbalanced datasets, where one class significantly outweighs the others. A model that predicts\n",
    "the majority class for every instance may achieve a high accuracy, but it provides limited value in such cases.\n",
    "Addressing: Consider using other metrics such as precision, recall, F1-score, or the area under the receiver operating characteristic curve (AUC-ROC) \n",
    "to assess performance more effectively. These metrics take into account false positives and false negatives, providing a better understanding of the \n",
    "model's behavior.\n",
    "\n",
    "Misleading Results in Rare Event Detection:\n",
    "Limitation: In scenarios where detecting rare events is crucial (e.g., fraud detection or disease diagnosis), accuracy can be misleading. A \n",
    "high accuracy might hide the model's inability to identify rare positive cases effectively.\n",
    "Addressing: Focus on metrics like precision, recall, F1-score, or AUC-ROC, which prioritize the model's performance on positive cases. Tuning the \n",
    "classification threshold or using techniques like oversampling or cost-sensitive learning can also improve rare event detection.\n",
    "\n",
    "Class Imbalance Mitigation:\n",
    "Limitation: Even if you use metrics like precision and recall, the class imbalance can still affect model evaluation. For example, a model that\n",
    "always predicts the minority class can have high precision but low recall.\n",
    "Addressing: Consider using techniques like stratified sampling, resampling (oversampling minority or undersampling majority), or generating synthetic \n",
    "samples (e.g., SMOTE) to balance the class distribution before evaluation. You can also use metrics like balanced accuracy, which accounts for class \n",
    "imbalance.\n",
    "\n",
    "Multi-Class Classification:\n",
    "Limitation: In multi-class problems, accuracy may not effectively capture the model's performance across all classes. It can be influenced by class \n",
    "sizes and may not reflect how well the model distinguishes between classes.\n",
    "Addressing: Use metrics like macro-averaged F1-score, weighted F1-score, or confusion matrices to assess the model's performance for each class \n",
    "individually and then aggregate the results. This provides insights into class-specific performance.\n",
    "\n",
    "Threshold Sensitivity:\n",
    "Limitation: Accuracy is not sensitive to the classification threshold, which determines the balance between precision and recall. Changing the \n",
    "threshold can significantly impact model behavior.\n",
    "Addressing: Examine the precision-recall curve or the ROC curve to select a threshold that aligns with your task's goals. Choose the threshold \n",
    "that optimizes the desired trade-off between precision and recall.\n",
    "\n",
    "Cost Sensitivity:\n",
    "Limitation: Accuracy treats all misclassifications equally, but in some applications, different types of errors have varying costs. For example, a \n",
    "false positive in medical diagnosis can have a different impact than a false negative.\n",
    "Addressing: Consider using cost-sensitive learning techniques or defining a custom loss function that accounts for the specific costs associated with\n",
    "different types of errors. This ensures that the model is optimized for the desired trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b212521-a066-43fb-bf31-ff122ea76e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
